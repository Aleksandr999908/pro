Рекомендации по построению классификатора животных для EcoAssist

Цель
Дать третью, уточняющую классификацию для «лиса/волк» на видеокадрах с фотоловушек (день/ночь, частичное попадание животного в кадр), поверх двух основных моделей EcoAssist (обычно: детекция MegaDetector → базовая классификация). EcoAssist (ныне AddaxAI) уже работает поверх MegaDetector и поддерживает видео/классификаторы — это наш якорь для интеграции.

---
Архитектура решения (с низкими требованиями)

1. Предобработка и выбор «кандидат-кадров»
   - Пробегаем видео редким шагом (напр., 1 кадр/сек) или используем лучший кадр из JSON MegaDetector.
   - Берём детектированные боксы «animal/person/vehicle» MegaDetector как ROI и добавляем паддинг контекста 10–20%.
   - Нормализация под домены день/ночь: отдельные пайплайны аугментаций.

2. Лёгкая видовая классификация (2 класса: fox/wolf + «другое/неуверенно»)
   - Модель: MobileNetV3-Large, EfficientNet-Lite0 или YOLOv8n-cls.
   - Обучение на ROI с паддингом и на full-frame как hard-negative.
   - Лосс: Focal Loss, баланс классов через sampler.

3. Тайм-агрегация по ролику
   - Majority vote или median по кадрам.
   - Temporal smoothing, пороги уверенности, класс «неуверенно».

4. Интеграция в EcoAssist / AddaxAI
   - Экспорт модели в ONNX.
   - Обёртка-постпроцессор: уточнение классификации после MegaDetector.
   - Добавление полей refine_species и refine_conf в JSON-результат.

---
Подготовка датасета

- Источник: папки «лисы/волки» по месяцам (видео).
- Скрипт извлечения кадров: 1 fps.
- Предфильтрация MegaDetector → только кадры с animal.
- Разметка: авто по папкам + ручная валидация hard cases.
- Третий класс other/unknown.
- Аугментации под день/ночь и частичные попадания.
- Hard mining: hard negatives (кусты, собаки), hard positives (ночные, смазанные).
- Разделение по месяцам/локациям для честной валидации.

---
Обучение и валидация

- PyTorch + timm / ultralytics-cls, экспорт в ONNX.
- Раздельные BN для день/ночь.
- Метрики: F1, balanced accuracy, AUROC.
- Калибровка вероятностей (temperature scaling).

---
Инференс на слабом железе

- ONNX Runtime (CPU), размер входа 224 px.
- Батчи 8–16, TTA минимальная.
- Скорость: десятки кадров/сек на обычном CPU.

---
Пост-логика принятия решения

1. Если P(fox) – P(wolf) > δ и обе > τ → класс.
2. При разнобое → temporal vote.
3. «Неуверенно» → без перезаписи, тег review.
4. Учет соседних кадров (скользящее окно 3–5 кадров).

---
Интеграция в EcoAssist

- После MegaDetector и базовой классификации.
- Поддержка ONNX, config.yaml, postprocess.py.
- Добавление уточняющей модели как третьего шага пайплайна.

---
Качество и приёмочные критерии

- F1 ≥ 0.92 для обоих классов.
- Снижение ручной правки учёных ≥ 30%.
- Отчёты по день/ночь, месяцам, частичности.
- Конфьюжн и PR-кривые для подбора порогов.

---
Риски и смягчения

- Сезонные изменения → стратификация + регулярное обновление.
- Частичные силуэты → класс «неуверенно».
- Похожие собаки → расширенный класс other.
- ИК-пересвет → CLAHE, Domain-Specific BN.

---
Следующие шаги

1. Скрипт выборки кадров из видеоархива.
2. Прогон MegaDetector и сбор кропов.
3. Baseline MobileNetV3-Large, Focal Loss, 10–20 эпох.
4. Hard-mining итерация и тонкая настройка.
5. Постпроцессор под EcoAssist/AddaxAI.


Единая инструкция: FGC-классификатор «Лиса vs Волк» для EcoAssist/AddaxAI
Версия: 1.0 | Фокус: низкие вычислительные требования, интеграция как «третья модель» в пайплайн EcoAssist/AddaxAI.
Схема пайплайна
Общий поток данных и модулей изображён ниже.
 
1. Подготовка данных и аннотирование (FGC)
1.1 Предварительная обработка: используйте ROI (bounding boxes) MegaDetector/EcoAssist для обрезки изображения вокруг животного с паддингом 10–20%. Это снижает фоновый шум и повышает устойчивость к частичной видимости.
1.2 Аннотирование ключевых точек (привилегированная информация): для обучающего набора создайте разметку keypoints для лисы и волка (напр., кончик носа, кончики ушей, основание хвоста). Эти признаки помогают модели учиться тонким морфологическим отличиям даже при окклюзиях или смазе.
1.3 Разделение на домены: при наличии IR-кадров создайте два поднабора — дневной (цвет) и ночной (ИК/моно), что позволяет обучить две лёгкие специализированные FGC-модели.
1.4 Контроль качества: стратифицируйте train/val/test по месяцам/локациям/камерам; добавьте класс other/unknown (пустые кадры, собаки и др.).
2. Выбор и обучение легковесной модели
2.1 Архитектура: EfficientNetV2‑S (или MobileNetV3-Large при ещё более жёстких ресурсах). Вход 224–256 px, mixed precision где доступно.
2.2 Функция потерь: ArcFace Loss (метрическое обучение) для повышения угловой разделимости классов. Альтернатива: CosFace/AM-Softmax. Кросс‑энтропию можно комбинировать, но основной акцент — на метрических лоссах.
2.3 Hard Negative Mining: после базового обучения найдите ошибки (волк→лиса, лиса→волк, умеренная уверенность), усильте их вес и переобучите несколько циклов. Добавляйте пустые/собаки как hard negatives.
2.4 Метрики и калибровка: F1, balanced accuracy, AUROC — раздельно по день/ночь и по «целый/частичный». Калибруйте вероятности (temperature scaling) для корректной пороговой логики.
3. Видеоконсистентность (временная агрегация)
3.1 Низкозатратное временное моделирование: извлекайте эмбеддинги кадров и применяйте Temporal Max Pooling по клипу с последующей классификацией/пороговой логикой. Такой подход устойчив к разнобою и дешёв по вычислениям.
3.2 Логика решений: задайте пороги τ (уверенность) и δ (зазор между классами). Если разброс велик — отдавайте класс «неуверенно».
4. Оптимизация и интеграция
4.1 Оптимизация: экспорт модели в ONNX; при наличии NVIDIA используйте TensorRT для ускорения инференса; для CPU/встроенных платформ — TensorFlow Lite. Поддерживайте один универсальный ONNX как эталон.
4.2 Развёртывание: оберните инференс в контейнеризированный REST API (Docker). Вход: ROI‑кроп (JPEG/PNG/Base64) и опциональные метаданные (день/ночь, месяц). Выход: класс (fox/wolf/unknown) + вероятность/уверенность.
4.3 Интеграция в EcoAssist/AddaxAI: подключите сервис как «третью модель». Триггеры вызова: (a) исходная классификация неуверенна или обобщённая (mammal/Canidae); (b) уверенность ниже порога. Сервис возвращает уточнение, которое записывается в дополнительные поля (напр., refine_species, refine_conf).
5. Лицензии и стек
Рекомендуемые открытые компоненты без ограничений на коммерцию: MegaDetector (MIT), timm/PyTorch (Apache-2.0/ BSD-3), ONNX Runtime (MIT), OpenCV (Apache-2.0), MMDetection/YOLOX/Detectron2 (Apache-2.0). Избегайте AGPL‑зависимостей в продакшене.
6. Приёмочные критерии
• F1 ≥ 0.92 по каждому классу на удержанном тесте; • снижение ручной разметки ≥ 30%; • отчёты по день/ночь, сезонности и частичности; • PR/ROC‑кривые и калибровочные графики; • latency/throughput на целевом железе.
7. Структура проекта
data/
  ├── videos/{month}/{fox|wolf}/...
  ├── frames/{video_id}/...
  ├── roi/{video_id}/...
  └── keypoints/{image_id}.json
models/
  ├── fgc_day.onnx
  ├── fgc_night.onnx
  └── config.yaml
service/
  ├── app.py (REST API)
  ├── Dockerfile
  └── requirements.txt
integrations/
  └── ecoassist_postprocess.py

8. Пример API (упрощённо)
POST /classify
{ "image": "<base64>", "domain": "day|night", "meta": {"month": "2025-10"} }

Ответ:
{ "class": "fox|wolf|unknown", "prob": 0.94, "refine_conf": 0.94 }
